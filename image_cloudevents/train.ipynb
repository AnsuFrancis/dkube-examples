{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "import sys,json, os\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "import kfp.compiler as compiler\n",
    "import kfp.dsl as dsl\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "\n",
    "from dkube.sdk import *\n",
    "from dkube.sdk.api import DkubeApi\n",
    "from dkube.sdk.rsrcs import DkubeModelmonitor\n",
    "from dkube.sdk.rsrcs.operator import DkubeCluster\n",
    "from dkube.sdk.rsrcs.modelmonitor import DatasetClass,ModelType,DriftAlgo\n",
    "from dkube.sdk.rsrcs.modelmonitor import DatasetFormat,DkubeModelmonitorAlert, TimeZone\n",
    "from dkube.sdk.rsrcs.modelmonitor import DataType, ChannelOrder, ImageDataSavedFileFormat\n",
    "\n",
    "job_class = os.getenv(\"DKUBE_JOB_CLASS\")\n",
    "if not job_class:\n",
    "    !{sys.executable} -m pip install kfp==1.4.0 kfp-server-api==1.2.0 --user >/dev/null\n",
    "\n",
    "# Set up font definitions for output\n",
    "class style:\n",
    "   RED = '\\033[91m\\033[1m'\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Configuration Variables from Resources Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONITOR_NAME = image_exp_config['MONITOR_NAME']\n",
    "INPUT_TRAIN_TYPE = image_exp_config['INPUT_TRAIN_TYPE']\n",
    "SERVING_DKUBE_USERNAME = image_exp_config['SERVING_DKUBE_USERNAME']\n",
    "TRAINING_DATASET = image_exp_config['TRAINING_DATASET']\n",
    "DKUBE_TRAINING_CODE_NAME = image_exp_config['DKUBE_TRAINING_CODE_NAME']\n",
    "SERVING_DKUBE_URL = image_exp_config['SERVING_DKUBE_URL']\n",
    "SERVING_DKUBE_TOKEN = image_exp_config['SERVING_DKUBE_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_deployment_running(deployment_id):\n",
    "    status = None\n",
    "    inference_url, inference = None, None\n",
    "    while True:\n",
    "        data = serving_api.get_deployment(deployment_id)\n",
    "        status = data.data.inferenceservice_deployment.parameters.generated.status.state\n",
    "        inference = data.data.inferenceservice_deployment.parameters.inference\n",
    "        inference_url = data.data.inferenceservice_deployment.parameters.generated.details.serving.servingurl\n",
    "        if status == \"RUNNING\":\n",
    "            break\n",
    "        print(\"waiting for deployment to be running\")\n",
    "        time.sleep(serving_api.wait_interval)\n",
    "    return inference, inference_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkube_preprocessing_op = components.load_component_from_file(\"/mnt/dkube/pipeline/components/preprocess/component.yaml\")\n",
    "dkube_training_op = components.load_component_from_file(\"/mnt/dkube/pipeline/components/training/component.yaml\")\n",
    "dkube_serving_op  = components.load_component_from_file(\"/mnt/dkube/pipeline/components/serving/component.yaml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = DkubeApi(URL=SERVING_DKUBE_URL, token=SERVING_DKUBE_TOKEN)\n",
    "client = kfp.Client(\n",
    "    host=os.getenv(\"KF_PIPELINES_ENDPOINT\"),\n",
    "    existing_token=SERVING_DKUBE_TOKEN,\n",
    "    namespace=SERVING_DKUBE_USERNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_training_dataset = TRAINING_DATASET\n",
    "training_program = DKUBE_TRAINING_CODE_NAME\n",
    "\n",
    "## Training stage inputs\n",
    "input_dataset_mount = ['/data']\n",
    "training_script = \"python image_cloudevents/training.py\"\n",
    "model_name = MONITOR_NAME\n",
    "output_model_mount = \"/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='xray-pipeline',\n",
    "    description='xray-training-pl'\n",
    ")\n",
    "def xray_pipeline(token):    \n",
    "    train       = dkube_training_op(container=json.dumps({\"image\": \"ocdr/dkube-datascience-tf-cpu:v2.0.0-10\"}),\n",
    "                                    framework=\"tensorflow\", version=\"2.0.0\",\n",
    "                                    program=str(training_program), \n",
    "                                    run_script=str(training_script),\n",
    "                                    datasets=json.dumps([str(input_training_dataset)]), \n",
    "                                    outputs=json.dumps([str(model_name)]),\n",
    "                                    input_dataset_mounts=json.dumps(input_dataset_mount),\n",
    "                                    output_mounts=json.dumps([str(output_model_mount)]),\n",
    "                                    auth_token=token)\n",
    "    \n",
    "    serving     = dkube_serving_op(model=train.outputs['artifact'], device='cpu',\n",
    "                                    name=MONITOR_NAME,\n",
    "                                    serving_image=json.dumps({\"image\": \"ocdr/tensorflowserver:2.0.0\"}),\n",
    "                                    auth_token=token, min_replicas = '1',\n",
    "                                    production=\"true\").after(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Pipeline Run to Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/1a79293b-ab9a-4e7d-addb-f4addd177d6a\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/a0e64f56-088d-44f2-8f81-3d3f11717d29\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for deployment to come up\n",
      "waiting for deployment to come up\n",
      "waiting for deployment to come up\n",
      "waiting for deployment to come up\n",
      "waiting for deployment to come up\n",
      "waiting for deployment to come up\n",
      "waiting for deployment to come up\n",
      "waiting for deployment to be running\n",
      "waiting for deployment to be running\n",
      "waiting for deployment to be running\n",
      "Enabling logs\n",
      "run ocdkube-image-mm-kf - waiting for completion, current state UPDATING\n",
      "run ocdkube-image-mm-kf - completed with state RUNNING and reason : \n",
      "Inference is up at URL:  https://10.142.0.76:32222/dkube/inference/ocdkube/-351310978:predict\n",
      "Stored 'image_exp_config' (dict)\n"
     ]
    }
   ],
   "source": [
    "# Check if the deployed model already exists\n",
    "# If not, create the pipeline run to train and deploy the model\n",
    "serving_api = DkubeApi(URL=SERVING_DKUBE_URL,token=SERVING_DKUBE_TOKEN)\n",
    "SERVING_DEPLOYMENT_ID = serving_api.get_deployment_id(name=MONITOR_NAME)\n",
    "\n",
    "# Create pipeline run name\n",
    "res = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
    "run_name = f\"{SERVING_DKUBE_USERNAME}-chest-xray-%s\"%res\n",
    "\n",
    "# Create and run the pipeline\n",
    "if not SERVING_DEPLOYMENT_ID:\n",
    "    client.create_run_from_pipeline_func(xray_pipeline, run_name=run_name, arguments={'token':SERVING_DKUBE_TOKEN})\n",
    "else:\n",
    "    print(f\"{style.BOLD}Deployment already exists, skipping training and deployment{style.END}\")\n",
    "\n",
    "# Wait for serving deployment to come up\n",
    "while True:\n",
    "    SERVING_DEPLOYMENT_ID = serving_api.get_deployment_id(name=MONITOR_NAME)\n",
    "    if SERVING_DEPLOYMENT_ID:\n",
    "        break\n",
    "    print(\"waiting for deployment to come up\")\n",
    "    time.sleep(serving_api.wait_interval)\n",
    "inference, INFERENCE_URL = wait_for_deployment_running(SERVING_DEPLOYMENT_ID)\n",
    "\n",
    "# Enable inference logs\n",
    "if not inference.enable_logs:\n",
    "    print(\"Enabling logs\")\n",
    "    serving = DkubeServing(user=SERVING_DKUBE_USERNAME, name=MONITOR_NAME)\n",
    "    serving.update_enable_logs(enable_logs=True)\n",
    "    serving_api.update_inference(serving)\n",
    "\n",
    "# Save deployment url & ID\n",
    "print(\"Inference is up at URL: \", INFERENCE_URL)\n",
    "image_exp_config['INFERENCE_URL'] = INFERENCE_URL\n",
    "image_exp_config['SERVING_DEPLOYMENT_ID'] = SERVING_DEPLOYMENT_ID\n",
    "%store image_exp_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fields used for Configuring the Monitor through the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mNote: The Deployment ID will be needed as an input during the UI-Based configuration process\u001b[0m\n",
      "\u001b[91m\u001b[1mDeployment ID (for Prefix Fields) = \u001b[0m xray-pipeline-9zqml-351310978\n"
     ]
    }
   ],
   "source": [
    "# Print the instructions for the required fields in the UI-Based setup\n",
    "print()\n",
    "print(f\"{style.BOLD}Note: The Deployment ID will be needed as an input during the UI-Based configuration process{style.END}\")\n",
    "print(f\"{style.RED}Deployment ID (for Prefix Fields) = {style.END}\", SERVING_DEPLOYMENT_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
