{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook file to accept a checkpoint file and continue training.\n",
    "\n",
    "This script expects the the checkpoint file to be written by the script `notebook-training.ipynb` as `./checkpoint/xray-weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "\n",
    "image_types = ('.jpg', 'jpeg', '.png', '.svg')\n",
    "\n",
    "# Ignore all warnings to clean up log file\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageData():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_data_from_dir(self, imagedir, grayscale=True, read_labels=False):\n",
    "        image_files = list()\n",
    "        for file_type in image_types:\n",
    "            image_files.extend(glob.glob(os.path.join(imagedir, \"**/*\" + file_type), recursive=True))\n",
    "        if len(image_files) == 0:\n",
    "            return None\n",
    "        images = []\n",
    "        for each_image_file in image_files:\n",
    "            if grayscale:\n",
    "                img = cv2.imread(each_image_file, cv2.IMREAD_GRAYSCALE)\n",
    "            else:\n",
    "                img = cv2.imread(each_image_file)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        train_x = np.asarray(images)\n",
    "        if read_labels:\n",
    "            csv_files = glob.glob(os.path.join(imagedir, \"**/*\" + \".csv\"), recursive=True)\n",
    "            label_data = pd.read_csv(csv_files[-1])\n",
    "            train_y = label_data.iloc[:,-1:].values\n",
    "            return train_x, train_y\n",
    "        else:\n",
    "            return train_x\n",
    "\n",
    "    def read_classification_data(self, datadir):\n",
    "        train_x = list()\n",
    "        train_y = list()\n",
    "        for dp, dn, filenames in os.walk(datadir):\n",
    "            if len(filenames) > 0:\n",
    "                current_class_data = self.read_data_from_dir(dp)\n",
    "                train_x.extend(current_class_data)\n",
    "                train_y.extend([os.path.basename(dp)] * current_class_data.shape[0])\n",
    "        if len(train_x) == 0:\n",
    "            return None\n",
    "        train_x = np.asarray(train_x)\n",
    "        train_y = np.asarray(train_y)\n",
    "        train_y_classes, train_y = np.unique(train_y, return_inverse=True)\n",
    "        return train_x, (train_y_classes, train_y)\n",
    "\n",
    "    def resize_images(self, images, new_shape):\n",
    "        resized_images = []\n",
    "        for each_image in images:\n",
    "            resized_images.append(cv2.resize(each_image, new_shape, interpolation= cv2.INTER_LINEAR))\n",
    "        resized_images = np.asarray(resized_images)\n",
    "        return resized_images\n",
    "\n",
    "# Set up image variables\n",
    "imd = ImageData()\n",
    "train_x, train_y = imd.read_classification_data(\"/data\")\n",
    "train_y_classes, train_y = train_y\n",
    "resized_train_x = imd.resize_images(train_x, (200,200))\n",
    "resized_train_x = resized_train_x.reshape(resized_train_x.shape[0], 200, 200, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "onehot = encoder.fit_transform(train_y.reshape(-1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up input hyperparameters\n",
    "NUM_EPOCHS = int(os.getenv(\"EPOCHS\", 5))\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Create & Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(200,200,1)),\n",
    "    tf.keras.layers.Conv2D(2, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='linear'),\n",
    "    tf.keras.layers.Dense(20, input_dim=5, activation='linear'),\n",
    "    tf.keras.layers.Dense(10, activation='linear'),\n",
    "    tf.keras.layers.Dense(4, activation='linear'),\n",
    "    tf.keras.layers.Dense(2, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_model()\n",
    "\n",
    "# Print the initial accuracy & loss\n",
    "loss, acc = model.evaluate(resized_train_x,onehot, verbose=False)\n",
    "print(\"Initial Accuracy = \", acc)\n",
    "print(\"Initial Loss =\", loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint weights into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./checkpoint/xray-weights\")\n",
    "\n",
    "# Print the updated accuracy & loss with previous weights\n",
    "loss, acc = model.evaluate(resized_train_x,onehot, verbose=False)\n",
    "print(\"Loaded Accuracy = \", acc)\n",
    "print(\"Loaded Loss =\", loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model & save TensorBoard events\n",
    "DKUBE_TENSORBOARD_DIR = os.environ.get('DKUBE_TENSORBOARD_DIR')\n",
    "model.fit(x=resized_train_x, y=onehot, epochs=NUM_EPOCHS, verbose=True, validation_split=0.1,\n",
    "  callbacks=[tf.keras.callbacks.TensorBoard(log_dir=DKUBE_TENSORBOARD_DIR)]\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
