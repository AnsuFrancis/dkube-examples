# Chest X-Ray Image Monitor Example (Pipeline-Based Workflow)

This example trains a model to identify pneumonia from chest x-rays.  The model is then deployed and used as the basis for monitoring with synthetic live data to demonstrate the DKube monitoring capability.

This workflow uses JupyterLab notebook scripts to set up the resources & create the model monitor, and a Kubeflow pipeline to train & deploy the model.  A separate readme file is available in this folder to create the monitor through the UI, in the same folder called [README-monitor-ui.md](README-monitor-ui.md)

- This example supports model deployment with a full DKube cluster (`serving cluster`) and model monitoring on either the same cluster or a seperate minimal DKube cluster (`monitoring cluster`).
  - **Serving cluster:** Where the production deployment will be running
  - **Monitoring cluster:** Where the model monitor will be running
  > **Note**: The serving and monitoring clusters can be same, but in that case the setup has to be a single **full** DKube setup

## Example Flow

- Create the necessary DKube resources
- Train and deploy the model
- Create a monitor
  - For separate serving and monitoring clusters
    - Add a serving cluster link on the monitoring cluster
    - Import the deployment onto the monitoring cluster
- Generate data for analysis by the monitor
  - Predict data: Inference inputs/outputs
  - Label data:  Dataset with Groundtruth values for the inferences generated above
  > **Note** In a production environment, the label data would be manually generated by experts in the domain.  In this example, we are creating the labeled data automatically.
- Cleanup the resources after the example is complete

## 1. Create the Code Repo

> **Note** If you have already created the Code Repo for this example from a previous set of steps, skip ahead to [Create JupyterLab Notebook](#2-create-and-launch-jupyterlab)

- Select `Code` menu on the left, then `+ Code`, and fill in the following fields:
  - **Name:** `chest-xray`  **(Or choose your own name as `<your-code-repo>`)**
  - **Source:** `Git`
  - **URL:** `https://github.com/oneconvergence/dkube-examples.git`
  - **Branch:** `training`
- Leave the rest of the fields at their current value
- `Add Code`

## 2. Create and Launch JupyterLab

> **Note** If you have already created the notebook for this example from a previous set of steps, skip ahead to [Create the Resources](#3-create-the-resources)

- Select `IDEs` menu on the left, then `+ JupyterLab`, and fill in the following fields:
  - **Basic Tab**
    - **Name:** `Choose an IDE name`
    - **Code:** *`<your-code-repo>`*  **(Chosen during Code repo creation)**
    - **Framework:** `tensorflow`
    - **Framework Version:** `2.0.0`
    - **Image:** `ocdr/dkube-datascience-tf-cpu-multiuser:v2.0.0-17`   **(This should be the default, but ensure that it is selected)**
- Leave the rest of the fields at their current value
- `Submit`

## 3. Create the Resources

This script creates the datasets and models required for training and monitoring. 

> **Note** This script will identify which resources have already been created, and create the ones that are necessary to continue

- Once the IDE is running, launch JupyterLab from the icon on the far right
- Navigate to <code>workspace/**\<your-code-repo\>**/chest-xray</code>
- Open `resources.ipynb`
> **Warning** Ensure that `Cleanup = False` in the last cell, since it may have been changed in a previous execution

> **Note** If you called your code repo something other than `chest-xray`, edit the following variable in the 3rd cell labeled `User-Defined Variables`:
  - `DKUBE_TRAINING_CODE_NAME` = *`<your-code-repo>`*
 
### Serving and Monitoring on Same Cluster

- If the serving and monitoring cluster are the same, no other fields needs to be changed, skip to [Run the Script](#run-the-script)

### Serving and Monitoring on Different Clusters

- If the monitoring cluster is separate from the serving cluster, you need to provide more information for cluster communication
  - `SERVING_CLUSTER_EXECUTION` = `False`
  - `SERVING_DKUBE_URL` = DKube access URL for the serving cluster, with the form
    - `https://<Serving Cluster Access IP Address>:32222/`
    > **Note** Ensure that there is a final `/` in the URL field
  - `MONITOR_DKUBE_URL `= DKube access URL from the monitoring cluster, with the form
  - `MONITORING_DKUBE_USERNAME` = Username on the monitoring cluster
  - `MONITORING_DKUBE_TOKEN` = Authentication token from the monitoring cluster, from the `Developer Settings` menu
    - `https://<Monitor Cluster Access IP Address>:32222/`
    > **Note** Ensure that there is a final `/` in the URL field
- If the Monitoring cluster already has a link to the Serving cluster from the DKube Clusters Operator screen
  - Get the name of the DKube cluster link and provide that name to the variable `SERVING_DKUBE_CLUSTER_NAME` <br><br>
- If the Monitoring cluster link has not been created by the Operator on the Monitoring cluster:
  - Leave the variable `SERVING_DKUBE_CLUSTER_NAME = ""`
  - In that case, the link will be created on the Monitoring cluster
  - The username identified in the `MONITORING_DKUBE_USERNAME` variable must have Operator privileges for this to work. If not, the script fill fail.
  - Leave the other fields at their current value

### Run the Script

- `Run` > `Run All Cells` from the top menu <br><br>

- The following resources will be created:
  - `chest-xray` Dataset on both the serving and monitoring cluster
  - `<your user name>-chest-xray` Model on the serving cluster
  - `<your user name>-chest-xray-s3` Dataset on the monitoring cluster

## 4. Train and Deploy the Model on Serving Cluster

This script trains and deploys a model on the serving cluster.  A Kubeflow Pipeline executes this step.

> **Note** This model will be used for the model monitor steps later in the example

> **Note** This step must be completed even if you did work with the example previously, since the deployment has specific requirements

- Open `train-and-deploy.ipynb`
- `Run All Cells`
- This creates and executes a pipeline in order to:
  - Preprocess the dataset and generate the training data or retraining data
  - Train with the generated dataset as an input, and create an output model
  - Deploy the generated model on a predict endpoint
- The pipeline will create a new version of the Model `<your user name>-chest-xray`
> **Note** Wait for the pipeline to complete before continuing

## 5. Create a Model Monitor

In order to monitor the deployed model, a monitor is created and launched.  This workflow executes this programatically through the DKube SDK. This can also be done through the UI by following a different example flow in this repo.

> **Note** The Monitor will use the Deployment that was created from the `train-and-deploy` script

- Open `modelmonitor.ipynb`
 
> **Warning** Ensure that `Cleanup = False` in the last cell, since it may have been changed in a previous execution
 
- `Run all Cells`
- This script will:
  - Add the right links and import the deployment if the monitoring is on a different cluster from the serving cluster
  - Create a new model monitor
- After the script has completed, the monitor `<your-user-name>-chest-xray` will be in the active state

## 6. Generate Data

Predict and Groundtruth datasets will be generated by this script, and will be used by the monitor to analyse the model execution.

- Open `data_generation.ipynb`
- In the 1st cell, update the number of data generation cycles to complete
  - `Run All Cells`
  - This will start to push the data based on the definitions generated in the previous `resources.ipynb` file

> **Note** Live data will be created on the MinIO server under the deployment ID.

## 7. Clean Up the Data when Complete

After you are done with the example, clean up the data by running the `Cleanup` cells in the `modelmonitor` and `resources` scripts

- Set the `Cleanup` variable to `True` in the last cell for each script, select that cell, and `Run Selected Cells` (not all cells)

> **Warning** Ensure that you restore the `Cleanup` variable to `False` after completion, or the scripts will not work on the next execution
