# INSURANCE EXAMPLE

- This example only supports predict dataset sources as **CloudEvents**. 
- This example also supports model deployment at a Full DKube cluster(serving cluster) and model monitoring on seperate minimal DKube cluster(monitoring cluster).
  - Serving cluster: Where production deployment will be running.
  - Monitoring cluster: Where model monitor will be running.
  - **Note**: Serving and monitoring cluster can be same but the setup has to be a single full Dkube setup.
- The notebooks in this example can be run inside or outside Dkube.

## Example Flow
- Create DKube resources. This includes Dataset and Model repo resources.
- Train a model for Insurance example using SkLearn and deploy the model for inference.
- Create a Modelmonitor. 
  - There is a mandatory requirement to deploy a model for this example
- Generate data for analysis by Modelmonitor
  - Predict data: Inference inputs/outputs
  - Label data:  Dataset with Groundtruth values for the inferences made above
  **In production, Label data would be generated by experts in the domain manually.**
- Cleanup resources after the example is complete

## Section 1: Create Dkube Resources

### Launch IDE (Inside Dkube)

#### Note: Follow the instructions if you are running Notebook IDE inside DKube. In case you are Notebook IDE outside DKube then clone the repo and checkout to monitoring branch and follow from step 4 of this section.

1. Add Code. Create Code Repo in Dkube with the following information
    - Name: **monitoring-examples**
    - Source: Git
    - URL: https://github.com/oneconvergence/dkube-examples.git
    - Branch : **monitoring-v3**
2. Create an IDE (JupyterLab)
   - Use Tensorflow framework with version 2.0.0
3. Click Submit.
4. Open Jupyterlab and from **workspace/monitoring-examples/insurance_cloudevents** open [resources.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/resources.ipynb) and fill the following details in the first cell.
    - In case of running the example notebook other than the serving setup, In the 1st cell, set RUNNING_IN_SAME to False and Fill the below details,
    - **SERVING_DKUBE_URL** = {DKube url of serving cluster}
    - **SERVING_DKUBE_USERNAME** = {DKube username of serving cluster}
    - **SERVING_DKUBE_TOKEN** = {DKube authentication token of serving cluster}
    - if there is a sperate monitoring cluster then also fill the below details, otherwise leave these value empty.
      - **MONITORING_DKUBE_USERNAME** = {DKube username of monitoring cluster}
      - **MONITORING_DKUBE_TOKEN** = {DKube authentication token of monitoring cluster}
      - **MONITORING_DKUBE_URL** = {DKube URL of monitoring cluster}
    - **MONITOR_NAME** = {model monitor name}
    - Cloudevents are stored in DKube Minio bucket. Enter the following details from the **Serving DKube Cluster**
      - **MINIO_KEY** = {MINIO access key of Dkube setup where the prediction deployment is running. See below}
      - **MINIO_SECRET_KEY** = {MINIO access secret key of Dkube setup where the prediction deployment is running. See below}
      - MINIO_KEY and MINIO_SECRET_KEY values will be filled automatically by the example with SDK call, these values can also be obtained by running the following commands on the DKube setup where the prediction deployment is running. Provide the creds manually if the user is neither PE nor Operator on the remote cluster.
        - DKube API. Fill in DKUBE_IP and TOKEN in the following curl command
          - `curl -X 'GET' \
              'https://DKUBE_IP:32222/dkube/v2/controller/v2/deployments/logstore' \
              -H 'accept: application/json' \
              -H 'Authorization: Bearer <TOKEN>'`
        - If you have access to Kubernetes, you can get the secrets by running the following commands
          - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_ACCESS_KEY_ID}" | base64 -d`
          - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_SECRET_ACCESS_KEY}" | base64 -d`
    - The following will be derived from the environment automatically if the notebook is running inside same Dkube IDE. Otherwise in case if the notebook is running locally or in other Dkube Setup , then please fill in, 
5. Run all the cells. This will create all the dkube resources required for this example automatically. In case of seperate serving and monitoring cluster, the required resources will be created on the respective cluster.
6. Once all the cells complete the run you will see the following resources will get created,
   1. `insurance-data` dataset on both serving and monitoring cluster.
   2. `insurance-training-data` dataset on serving cluster.
   3. `insurance-mm-kf` model on serving cluster.
   4. `insurance-mm-kf-s3` dataset on monitoring cluster.


## Section 2: Insurance Model Training (Required to deploy model)

#### Note: This uses DKube Runs, Kubeflow Pipelines and KfServing. It requires full Dkube installed. 

1. From **workspace/monitoring-examples/insurance_cloudevents** open **train.ipynb** to build the pipeline.
2. The pipeline includes preprocessing, training and serving stages. Run all cells
     - **preprocessing**: the preprocessing stage generates the dataset (either training-data or retraining-data) depending on user choice.
     - **training**: the training stage takes the generated dataset as input, train a sgd model and outputs the model.
     - **serving**: The serving stage takes the generated model and serve it with a predict endpoint for inference.
3. Verify that the pipeline has created the following resources
     - Datasets: 'insurance-training-data' with version v2.
     - Model: 'insurance-model' with version v2

## Section 3: Modelmonitoring
DKube provides Python SDK for creating a modelmonitor programmatically. You could also choose to create a modelmonitor from the DKube UI. This example cuurently support creating monitor using SDK. 

1. From **workspace/monitoring-examples/insurance_cloudevents** open [modelmonitor.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/modelmonitor.ipynb) and run all the cells. New model monitor will be created. If monitoring cluster details are given, it will also add the serving cluster in the monitoring cluster, and import the deployment on monitoring cluster. 
2. Predict and Groundtruth dataset data will be generated by Data Generation step and will be utilised by modelmonitor.
3. After the completion of the notebook, you will see the model monitor `insurance-mm-kf` in active state.

## Section 4: Data Generation
1. Open [data_generation.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/data_generation.ipynb) notebook for making predictions with the deployemnt endpoint and generate groundtruth datasets.
2. In 1st cell, Update Frequency according to what you set in Modelmonitor for Drift. For eg: for 5 minutes, specify it as `5m` and to specify the frequency in hours use `5h` for 5 hours interval.
3. Then Run All Cells. It will start Pushing the data. It uses the data definitions specified in resources.ipynb file.

**Note:** Livedata will be created on the MINIO under deployment id. In the case of minimal DKube, we will create on the serving cluster minio where deployments are running.

## Section 5: SMTP Settings (Optional)
Configure your SMTP server settings on Operator screen. This is optional. If SMTP server is not configured, no email alerts will be generated.

## Section 6: Cleanup
1. After your experiment is complete, 
   - Open [modelmonitor.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/modelmonitor.ipynb) and set CLEANUP=True in last Cleanup cell and run.
   - Open [resources.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/resources.ipynb) and set CLEANUP=True in last Cleanup cell and run.


